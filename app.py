from flask import Flask, request, jsonify
import re
import pickle
import pandas as pd
import numpy as np
import logging
import os

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞
try:
    model_path = os.path.join(os.path.dirname(__file__), 'xgboost_token_model.pkl')
    with open(model_path, 'rb') as f:
        model_artifacts = pickle.load(f)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    model_artifacts = None

def parse_token_data(text):
    """
    –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∞ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
    """
    try:
        token_data = {}
        
        # Market Cap - "MC: $136.8K"
        mcap_match = re.search(r'MC:\s*\$?([0-9,.]+)([KMB]?)', text, re.IGNORECASE)
        if mcap_match:
            value_str = mcap_match.group(1).replace(',', '')
            value = float(value_str)
            unit = mcap_match.group(2).upper()
            if unit == 'K':
                value *= 1000
            elif unit == 'M':
                value *= 1000000
            elif unit == 'B':
                value *= 1000000000
            token_data['market_cap'] = value
        else:
            token_data['market_cap'] = 0
        
        # Liquidity - "Liq: $42.4K"
        liq_match = re.search(r'Liq:\s*\$?([0-9,.]+)([KMB]?)', text, re.IGNORECASE)
        if liq_match:
            value_str = liq_match.group(1).replace(',', '')
            value = float(value_str)
            unit = liq_match.group(2).upper()
            if unit == 'K':
                value *= 1000
            elif unit == 'M':
                value *= 1000000
            elif unit == 'B':
                value *= 1000000000
            token_data['liquidity'] = value
        else:
            token_data['liquidity'] = 0
        
        # Volume - –±–µ—Ä–µ–º 1min volume: "Volume: $12,129.12"
        vol_1min_match = re.search(r'1 min:.*?Volume:\s*\$?([0-9,.]+)', text, re.DOTALL)
        if vol_1min_match:
            value_str = vol_1min_match.group(1).replace(',', '')
            token_data['volume_1min'] = float(value_str)
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç 1min, –±–µ—Ä–µ–º 5min
            vol_5min_match = re.search(r'5 min:.*?Volume:\s*\$?([0-9,.]+)', text, re.DOTALL)
            if vol_5min_match:
                value_str = vol_5min_match.group(1).replace(',', '')
                token_data['volume_1min'] = float(value_str)
            else:
                token_data['volume_1min'] = 0
        
        # Last Volume - –Ω–µ—Ç –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, —Å—Ç–∞–≤–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        token_data['last_volume'] = token_data['volume_1min']  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π –æ–±—ä–µ–º
        token_data['last_volume_multiplier'] = 1.0
        
        # Token Age - "Token age: 25m"
        age_match = re.search(r'Token age:\s*(?:(\d+)h\s*)?(?:(\d+)m\s*)?(?:(\d+)s\s*)?', text, re.IGNORECASE)
        if age_match:
            hours = int(age_match.group(1)) if age_match.group(1) else 0
            minutes = int(age_match.group(2)) if age_match.group(2) else 0
            seconds = int(age_match.group(3)) if age_match.group(3) else 0
            total_minutes = hours * 60 + minutes + seconds / 60
            token_data['token_age_numeric'] = total_minutes
        else:
            token_data['token_age_numeric'] = 0
        
        # –î–µ—Ä–∂–∞—Ç–µ–ª–∏ –ø–æ —Ü–≤–µ—Ç–∞–º - "üü¢: 8 | üîµ: 5 | üü°: 12 | ‚≠ïÔ∏è: 42"
        holders_match = re.search(r'üü¢:\s*(\d+)\s*\|\s*üîµ:\s*(\d+)\s*\|\s*üü°:\s*(\d+)\s*\|\s*‚≠ïÔ∏è:\s*(\d+)', text)
        if holders_match:
            token_data['green_holders'] = int(holders_match.group(1))
            token_data['blue_holders'] = int(holders_match.group(2))
            token_data['yellow_holders'] = int(holders_match.group(3))
            token_data['circle_holders'] = int(holders_match.group(4))
        else:
            token_data['green_holders'] = 0
            token_data['blue_holders'] = 0
            token_data['yellow_holders'] = 0
            token_data['circle_holders'] = 0
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –¥–µ—Ä–∂–∞—Ç–µ–ª–∏ - "ü§°: 0 | üåû: 0 | üåó: 0 | üåö: 3"
        special_holders_match = re.search(r'ü§°:\s*(\d+)\s*\|\s*üåû:\s*(\d+)\s*\|\s*üåó:\s*(\d+)\s*\|\s*üåö:\s*(\d+)', text)
        if special_holders_match:
            token_data['clown_holders'] = int(special_holders_match.group(1))
            token_data['sun_holders'] = int(special_holders_match.group(2))
            token_data['half_moon_holders'] = int(special_holders_match.group(3))
            token_data['dark_moon_holders'] = int(special_holders_match.group(4))
        else:
            token_data['clown_holders'] = 0
            token_data['sun_holders'] = 0
            token_data['half_moon_holders'] = 0
            token_data['dark_moon_holders'] = 0
        
        # Total holders - "Total: 168"
        total_holders_match = re.search(r'Total:\s*(\d+)', text)
        if total_holders_match:
            token_data['total_holders'] = int(total_holders_match.group(1))
        else:
            token_data['total_holders'] = (
                token_data['green_holders'] + token_data['blue_holders'] + 
                token_data['yellow_holders'] + token_data['circle_holders']
            )
        
        # Top 10 percent - "Top 10: 23%"
        top10_match = re.search(r'Top 10:\s*([0-9.]+)%', text)
        if top10_match:
            token_data['top10_percent'] = float(top10_match.group(1))
        else:
            token_data['top10_percent'] = 50.0
        
        # Current/Initial percentages - "Current/Initial: 16.76% / 98.87%"
        current_initial_match = re.search(r'Current/Initial:\s*([0-9.]+)%\s*/\s*([0-9.]+)%', text)
        if current_initial_match:
            token_data['total_now_percent'] = float(current_initial_match.group(1))
            token_data['total_percent'] = float(current_initial_match.group(2))
        else:
            token_data['total_percent'] = 100.0
            token_data['total_now_percent'] = 50.0
        
        # Dev holds - "Dev current balance: 0%"
        dev_match = re.search(r'Dev current balance:\s*([0-9.]+)%', text)
        if dev_match:
            token_data['dev_holds_percent'] = float(dev_match.group(1))
        else:
            token_data['dev_holds_percent'] = 0.0
        
        # –ü–æ–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –Ω–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ - –∑–∞–ø–æ–ª–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        token_data['insiders_count'] = 0
        token_data['insiders_percent'] = 0.0
        token_data['snipers_count'] = 0
        token_data['bundle_total'] = 0
        token_data['bundle_supply_percent'] = 0.0
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
        token_data['volume_to_liquidity'] = float(
            np.log1p(token_data['volume_1min']) / np.log1p(token_data['liquidity'] + 1)
            if token_data['liquidity'] > 0 else 0
        )
        
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        token_data['log_market_cap'] = float(np.log1p(token_data['market_cap']))
        token_data['log_liquidity'] = float(np.log1p(token_data['liquidity']))
        token_data['log_volume_1min'] = float(np.log1p(token_data['volume_1min']))
        token_data['log_last_volume'] = float(np.log1p(token_data['last_volume']))
        
        # –ö–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è –¥–µ—Ä–∂–∞—Ç–µ–ª–µ–π
        token_data['holder_concentration'] = int(
            token_data['green_holders'] + token_data['blue_holders'] + 
            token_data['yellow_holders'] + token_data['circle_holders']
        )
        
        # –†–∏—Å–∫-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        token_data['total_risk_percent'] = float(
            token_data['dev_holds_percent'] + token_data['insiders_percent']
        )
        
        return convert_to_json_serializable(token_data)
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
        return {}

def convert_to_json_serializable(obj):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç numpy —Ç–∏–ø—ã –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ Python —Ç–∏–ø—ã –¥–ª—è JSON"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_json_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_json_serializable(item) for item in obj]
    else:
        return obj

def predict_token_success(token_data):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞ (—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö)
    """
    if model_artifacts is None:
        return {'error': '–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}
    
    try:
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df_new = pd.DataFrame([token_data])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
        for col in model_artifacts['feature_names']:
            if col not in df_new.columns:
                df_new[col] = 0
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        df_new = df_new[model_artifacts['feature_names']]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏–º–ø—É—Ç–µ—Ä
        df_imputed = pd.DataFrame(
            model_artifacts['imputer'].transform(df_new), 
            columns=model_artifacts['feature_names']
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
        raw_probability = float(model_artifacts['model'].predict_proba(df_imputed)[0, 1])
        
        # üîÑ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –î–†–ò–§–¢–ê: –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
        probability = 1.0 - raw_probability
        prediction = int(probability >= 0.5)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence_score = abs(probability - 0.5) * 2
        if confidence_score > 0.8:
            confidence_level = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
        elif confidence_score > 0.6:
            confidence_level = "–í—ã—Å–æ–∫–∞—è"
        elif confidence_score > 0.4:
            confidence_level = "–°—Ä–µ–¥–Ω—è—è"
        else:
            confidence_level = "–ù–∏–∑–∫–∞—è"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
        if probability >= 0.7:
            recommendation = "–ü–û–ö–£–ü–ê–¢–¨"
            risk_level = "–ù–∏–∑–∫–∏–π"
        elif probability >= 0.5:
            recommendation = "–ò–ó–£–ß–ò–¢–¨"
            risk_level = "–°—Ä–µ–¥–Ω–∏–π"
        else:
            recommendation = "–ü–†–û–ü–£–°–¢–ò–¢–¨"
            risk_level = "–í—ã—Å–æ–∫–∏–π"
        
        result = {
            'prediction': '–î–ê' if prediction == 1 else '–ù–ï–¢',
            'probability': round(float(probability), 4),
            'probability_percent': f"{probability*100:.1f}%",
            'confidence_level': confidence_level,
            'confidence_score': round(float(confidence_score), 4),
            'recommendation': recommendation,
            'risk_level': risk_level,
            'threshold_conservative': '–î–ê' if probability >= 0.7 else '–ù–ï–¢',
            'threshold_optimal': '–î–ê' if probability >= 0.5 else '–ù–ï–¢',
            'threshold_aggressive': '–î–ê' if probability >= 0.3 else '–ù–ï–¢',
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            'model_info': {
                'raw_probability': round(raw_probability, 4),
                'corrected_probability': round(probability, 4),
                'drift_correction': True,
                'data_format': 'new_format_optimized'
            }
        }
        
        return convert_to_json_serializable(result)
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
        return {'error': f'–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}'}

@app.route('/health', methods=['GET'])
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ API"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model_artifacts is not None,
        'version': '2.1',
        'optimized_for': 'new_data_format',
        'drift_correction': 'enabled',
        'environment': os.environ.get('RAILWAY_ENVIRONMENT', 'unknown')
    })

@app.route('/predict', methods=['POST'])
def predict():
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    try:
        data = request.get_json()
        
        if not data or 'text' not in data:
            return jsonify({'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–µ—Ä–µ–¥–∞—Ç—å –ø–æ–ª–µ "text" —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞'}), 400
        
        # –ü–∞—Ä—Å–∏–º —Ç–µ–∫—Å—Ç
        token_data = parse_token_data(data['text'])
        
        if not token_data:
            return jsonify({'error': '–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω–∞'}), 400
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        result = predict_token_success(token_data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if data.get('include_parsed_data', False):
            result['parsed_data'] = convert_to_json_serializable(token_data)
        
        return jsonify(result)
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ API: {e}")
        return jsonify({'error': f'–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {str(e)}'}), 500

@app.route('/test', methods=['GET'])
def test():
    """–¢–µ—Å—Ç–æ–≤—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
    test_text = """üé≤ $PVE | President vs Elon

3nuogKUQuxfxjCRud7Bpm5a9Q7eT7mxpFGNe9WeNbonk

‚è≥ Token age:  25m  | üëÅ 14
‚îú MC: $136.8K
‚îú Liq: $42.4K / SOL pooled: 111.02
‚îî ATH: $134.6K (-4% / 4s)

1 min:
‚îú Volume: $12,129.12
‚îú Buy volume ($): $6,446.81
‚îú Sell volume ($): $5,682.31
‚îú Buys: 165
‚îî Sells: 177

5 min:
‚îú Volume: $71,175.80
‚îú Buy volume ($): $46,124.03
‚îú Sell volume ($): $25,051.77
‚îú Buys: 585
‚îî Sells: 448

üéØ First 70 buyers:
üåöüåöüåöüü°üü°üü°üü°üü°üü°üü°
üü°üü¢üü°‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
‚≠ïÔ∏èüü¢üîµüîµüü¢üü¢‚≠ïÔ∏èüü°üü°‚≠ïÔ∏è
‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏èüü¢üü¢
üîµ‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏èüü¢üîµ‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è
üîµ‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏èüü¢‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏è‚≠ïÔ∏èüü°

‚îú üü¢: 8 | üîµ: 5 | üü°: 12 | ‚≠ïÔ∏è: 42
‚îú ü§°: 0 | üåû: 0 | üåó: 0 | üåö: 3
‚îú Current/Initial: 16.76% / 98.87%

üë• Holders:
‚îú Total: 168
‚îú Freshies: 8.8% 1D | 87% 7D
‚îú Top 10: 23%
üí∞ Top 10 Holding (%)
15.82 | 2.48 | 2.42 | 2.42 | 2.41 | 2.38 | 2.31 | 2.29 | 2.26 | 2.22

üòé Dev
‚îú Dev current balance: 0%
‚îî Dev SOL balance: 0 SOL

üîí Security:
‚îú NoMint: üü¢
‚îú Blacklist: üü¢
‚îú Burnt: üü¢
‚îú Dev Sold: üü¢
‚îî Dex Paid: üî¥"""
    
    token_data = parse_token_data(test_text)
    result = predict_token_success(token_data)
    result['parsed_data'] = convert_to_json_serializable(token_data)
    
    return jsonify(result)

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('RAILWAY_ENVIRONMENT') != 'production'
    
    print("üöÄ Token Prediction API v2.1")
    print(f"üìç –ü–æ—Ä—Ç: {port}")
    print(f"üîÑ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö")
    print(f"üß™ –¢–µ—Å—Ç: /test")
    print(f"‚ù§Ô∏è  –°—Ç–∞—Ç—É—Å: /health")
    
    app.run(host='0.0.0.0', port=port, debug=debug)
